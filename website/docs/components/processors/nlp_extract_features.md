---
title: nlp_extract_features
slug: nlp_extract_features
type: processor
status: beta
categories: ["Machine Learning","NLP"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Performs feature extraction using a Hugging Face ðŸ¤— NLP pipeline with an ONNX Runtime model.

Introduced in version v1.3.0 (huggingbento).


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
label: ""
nlp_extract_features:
  pipeline_name: "" # No default (optional)
  model_path: /model_repository
  model_download_options: {}
  normalization: false
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
label: ""
nlp_extract_features:
  pipeline_name: "" # No default (optional)
  model_path: /model_repository
  onnx_library_path: /usr/lib/onnxruntime.so
  onnx_filename: ""
  enable_model_download: false
  model_download_options:
    model_repository: ""
  normalization: false
```

</TabItem>
</Tabs>

### Feature Extraction
Feature extraction is the task of extracting features learnt in a model.This processor runs a feature extraction model against batches of text data, returning a model's multidimensional representation of said featuresin tensor/float64 format.
This component uses [Hugot](https://github.com/knights-analytics/hugot), a library that provides an interface for running [Open Neural Network Exchange (ONNX) models](https://onnx.ai/onnx/intro/) and transformer pipelines, with a focus on NLP tasks.

Currently, [HuggingBento only implements](https://github.com/knights-analytics/hugot/tree/main?tab=readme-ov-file#implemented-pipelines):
	
- [featureExtraction](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)
- [textClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextClassificationPipeline)
- [tokenClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TokenClassificationPipeline)

### What is a pipeline?
From [HuggingFace docs](https://huggingface.co/docs/transformers/en/main_classes/pipelines):
> A pipeline in ðŸ¤— Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

:::warning
While, only models in [ONNX](https://onnx.ai/) format are supported, exporting existing formats to ONNX is both possible and straightforward in most standard ML libraries. For more on this, check out the [ONNX conversion docs](https://onnx.ai/onnx/intro/converters.html). 
Otherwise, check out using [HuggingFace Optimum](https://huggingface.co/docs/optimum/en/exporters/onnx/usage_guides/export_a_model) for easy model conversion.
:::


## Fields

### `pipeline_name`

Name of the pipeline. Defaults to uuid_v4() if not set


Type: `string`  

### `model_path`

Path to the ONNX model directory. If `enable_model_download` is `true`, the model will be downloaded here.


Type: `string`  
Default: `"/model_repository"`  

```yml
# Examples

model_path: /path/to/models/my_model.onnx
```

### `onnx_library_path`

The location of the ONNX Runtime dynamic library.


Type: `string`  
Default: `"/usr/lib/onnxruntime.so"`  

### `onnx_filename`

The filename of the model to run. Only necessary to specify when multiple .onnx files are present.


Type: `string`  
Default: `""`  

```yml
# Examples

onnx_filename: model.onnx
```

### `enable_model_download`

If enabled, attempts to download an ONNX Runtime compatible model from HuggingFace specified in `model_name`.


Type: `bool`  
Default: `false`  

### `model_download_options`

Sorry! This field is missing documentation.


Type: `object`  

### `model_download_options.model_repository`

The name of the huggingface model repository.


Type: `string`  
Default: `""`  

```yml
# Examples

model_repository: KnightsAnalytics/distilbert-NER

model_repository: KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english

model_repository: sentence-transformers/all-MiniLM-L6-v2
```

### `normalization`

Whether to apply normalization in the feature extraction pipeline.


Type: `bool`  
Default: `false`  


